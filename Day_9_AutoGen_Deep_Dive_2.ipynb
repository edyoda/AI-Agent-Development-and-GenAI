{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqGS0RcbxTwDIYW0Meg8XD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edyoda/AI-Agent-Development-and-GenAI/blob/main/Day_9_AutoGen_Deep_Dive_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BqQuQ7bZGgy",
        "outputId": "3318419b-50f9-4a78-d397-c164b2e15873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen\n",
            "  Downloading autogen-0.8.6-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pyautogen==0.8.6 (from autogen)\n",
            "  Downloading pyautogen-0.8.6-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.6->autogen) (4.9.0)\n",
            "Collecting asyncer==0.0.8 (from pyautogen==0.8.6->autogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from pyautogen==0.8.6->autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from pyautogen==0.8.6->autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.6->autogen) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.6->autogen) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.6->autogen) (2.11.2)\n",
            "Collecting python-dotenv (from pyautogen==0.8.6->autogen)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.8.6->autogen) (3.0.1)\n",
            "Collecting tiktoken (from pyautogen==0.8.6->autogen)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.6->autogen) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.6->autogen) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen==0.8.6->autogen) (4.13.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen==0.8.6->autogen) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen==0.8.6->autogen) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->pyautogen==0.8.6->autogen) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.6->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.6->autogen) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.6->autogen) (0.4.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.8.6->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.8.6->autogen) (2.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->pyautogen==0.8.6->autogen) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->pyautogen==0.8.6->autogen) (3.4.1)\n",
            "Downloading autogen-0.8.6-py3-none-any.whl (13 kB)\n",
            "Downloading pyautogen-0.8.6-py3-none-any.whl (734 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.2/734.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, diskcache, tiktoken, docker, asyncer, pyautogen, autogen\n",
            "Successfully installed asyncer-0.0.8 autogen-0.8.6 diskcache-5.6.3 docker-7.1.0 pyautogen-0.8.6 python-dotenv-1.1.0 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install autogen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogen-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGZ5-Ou5ZVvF",
        "outputId": "e99d9248-a55b-4868-9785-3f3af9b6d7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen-core\n",
            "  Downloading autogen_core-0.5.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting jsonref~=1.1.0 (from autogen-core)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core) (1.31.1)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core) (11.1.0)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core) (4.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core) (0.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core) (3.21.0)\n",
            "Downloading autogen_core-0.5.1-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Installing collected packages: jsonref, autogen-core\n",
            "Successfully installed autogen-core-0.5.1 jsonref-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogen-agentchat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqzRc75naD81",
        "outputId": "1d3bbe5d-bbaf-417f-a151-5c6e2911cdcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen-agentchat\n",
            "  Downloading autogen_agentchat-0.5.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: autogen-core==0.5.1 in /usr/local/lib/python3.11/dist-packages (from autogen-agentchat) (0.5.1)\n",
            "Requirement already satisfied: jsonref~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (1.1.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (1.31.1)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (11.1.0)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-agentchat) (4.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-agentchat) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-agentchat) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-agentchat) (0.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-agentchat) (3.21.0)\n",
            "Downloading autogen_agentchat-0.5.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: autogen-agentchat\n",
            "Successfully installed autogen-agentchat-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogen-ext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itZ4F1xqZ6In",
        "outputId": "5ea93ef9-beef-4fd9-9c87-d86c10137b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen-ext\n",
            "  Downloading autogen_ext-0.5.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: autogen-core==0.5.1 in /usr/local/lib/python3.11/dist-packages (from autogen-ext) (0.5.1)\n",
            "Requirement already satisfied: jsonref~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext) (1.1.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.27.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext) (1.31.1)\n",
            "Requirement already satisfied: pillow>=11.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext) (11.1.0)\n",
            "Requirement already satisfied: protobuf~=5.29.3 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from autogen-core==0.5.1->autogen-ext) (4.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext) (8.6.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-ext) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-ext) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.5.1->autogen-ext) (0.4.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.5.1->autogen-ext) (3.21.0)\n",
            "Downloading autogen_ext-0.5.1-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.9/259.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: autogen-ext\n",
            "Successfully installed autogen-ext-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "umoZRRZZZbis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managing State\n",
        "So far, we have discussed how to build components in a multi-agent application - agents, teams, termination conditions. In many cases, it is useful to save the state of these components to disk and load them back later. This is particularly useful in a web application where stateless endpoints respond to requests and need to load the state of the application from persistent storage.\n",
        "\n",
        "In this notebook, we will discuss how to save and load the state of agents, teams, and termination conditions.\n",
        "\n",
        "**Saving and Loading Agents**"
      ],
      "metadata": {
        "id": "DHu9khZMZpj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.conditions import MaxMessageTermination\n",
        "from autogen_agentchat.messages import TextMessage\n",
        "from autogen_agentchat.teams import RoundRobinGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_core import CancellationToken\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n",
        "\n",
        "assistant_agent = AssistantAgent(\n",
        "    name=\"assistant_agent\",\n",
        "    system_message=\"You are a helpful assistant\",\n",
        "    model_client=model_client,\n",
        ")\n",
        "\n",
        "# Use asyncio.run(...) when running in a script.\n",
        "response = await assistant_agent.on_messages(\n",
        "    [TextMessage(content=\"Write a 3 line poem on lake tangayika\", source=\"user\")], CancellationToken()\n",
        ")\n",
        "print(response.chat_message)\n",
        "await model_client.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzp6nAu8ZisS",
        "outputId": "deac35da-070a-4db6-96aa-c6ea069dda2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source='assistant_agent' models_usage=RequestUsage(prompt_tokens=28, completion_tokens=40) metadata={} content=\"In Tanganyika's depths, a mirror wide and deep,  \\nAncient waters whisper secrets fish and shadows keep,  \\nCradled by the mountains, where the earth and sky meet.  \" type='TextMessage'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_state = await assistant_agent.save_state()\n",
        "print(agent_state)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEChyvkZaHe2",
        "outputId": "1823ad0c-7b98-405b-8b56-ba699c5bc54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'type': 'AssistantAgentState', 'version': '1.0.0', 'llm_context': {'messages': [{'content': 'Write a 3 line poem on lake tangayika', 'source': 'user', 'type': 'UserMessage'}, {'content': \"In Tanganyika's depths, a mirror wide and deep,  \\nAncient waters whisper secrets fish and shadows keep,  \\nCradled by the mountains, where the earth and sky meet.  \", 'thought': None, 'source': 'assistant_agent', 'type': 'AssistantMessage'}]}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n",
        "\n",
        "new_assistant_agent = AssistantAgent(\n",
        "    name=\"assistant_agent\",\n",
        "    system_message=\"You are a helpful assistant\",\n",
        "    model_client=model_client,\n",
        ")\n",
        "await new_assistant_agent.load_state(agent_state)\n",
        "\n",
        "# Use asyncio.run(...) when running in a script.\n",
        "response = await new_assistant_agent.on_messages(\n",
        "    [TextMessage(content=\"What was the last line of the previous poem you wrote\", source=\"user\")], CancellationToken()\n",
        ")\n",
        "print(response.chat_message)\n",
        "await model_client.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MGYV2KebUdP",
        "outputId": "19da92ae-2573-4e83-b24a-522bcf70ec4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source='assistant_agent' models_usage=RequestUsage(prompt_tokens=90, completion_tokens=27) metadata={} content='The last line of the poem I wrote was:  \\n\"Cradled by the mountains, where the earth and sky meet.\"' type='TextMessage'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and Loading Teams\n",
        "We can get the state of a team by calling save_state method on the team and load it back by calling load_state method on the team.\n",
        "\n",
        "When we call save_state on a team, it saves the state of all the agents in the team.\n",
        "\n",
        "We will begin by creating a simple RoundRobinGroupChat team with a single agent and ask it to write a poem."
      ],
      "metadata": {
        "id": "QjIMPECqccUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-2024-08-06\")\n",
        "\n",
        "# Define a team.\n",
        "assistant_agent = AssistantAgent(\n",
        "    name=\"assistant_agent\",\n",
        "    system_message=\"You are a helpful assistant\",\n",
        "    model_client=model_client,\n",
        ")\n",
        "agent_team = RoundRobinGroupChat([assistant_agent], termination_condition=MaxMessageTermination(max_messages=2))\n",
        "\n",
        "# Run the team and stream messages to the console.\n",
        "stream = agent_team.run_stream(task=\"Write a beautiful poem 3-line about lake tangayika\")\n",
        "\n",
        "# Use asyncio.run(...) when running in a script.\n",
        "await Console(stream)\n",
        "\n",
        "# Save the state of the agent team.\n",
        "team_state = await agent_team.save_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVRa_Ws5b7bN",
        "outputId": "c2c4ab23-1a8d-4f2b-c619-f2956b484c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "Write a beautiful poem 3-line about lake tangayika\n",
            "---------- assistant_agent ----------\n",
            "Beneath the sun's embrace, Tanganyika gleams,  \n",
            "Whispers of ancient waters weave through dreams,  \n",
            "Eternal beauty, where earth and sky convene.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "await agent_team.reset()\n",
        "stream = agent_team.run_stream(task=\"What was the last line of the poem you wrote?\")\n",
        "await Console(stream)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hj2dd5B-esyj",
        "outputId": "01678d06-21cd-4256-fd17-2a5f76cf2a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "What was the last line of the poem you wrote?\n",
            "---------- assistant_agent ----------\n",
            "I’m sorry, but I don’t have the ability to remember past interactions or poems. However, I’d be happy to help you write a new one if you’d like!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='What was the last line of the poem you wrote?', type='TextMessage'), TextMessage(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=28, completion_tokens=37), metadata={}, content='I’m sorry, but I don’t have the ability to remember past interactions or poems. However, I’d be happy to help you write a new one if you’d like!', type='TextMessage')], stop_reason='Maximum number of messages 2 reached, current message count: 2')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(team_state)\n",
        "\n",
        "# Load team state.\n",
        "await agent_team.load_state(team_state)\n",
        "stream = agent_team.run_stream(task=\"What was the last line of the poem you wrote?\")\n",
        "await Console(stream)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1205YkXmey8w",
        "outputId": "f7a83083-be70-49f1-d6ec-efb8f318aa93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'type': 'TeamState', 'version': '1.0.0', 'agent_states': {'assistant_agent': {'type': 'ChatAgentContainerState', 'version': '1.0.0', 'agent_state': {'type': 'AssistantAgentState', 'version': '1.0.0', 'llm_context': {'messages': [{'content': 'Write a beautiful poem 3-line about lake tangayika', 'source': 'user', 'type': 'UserMessage'}, {'content': \"Beneath the sun's embrace, Tanganyika gleams,  \\nWhispers of ancient waters weave through dreams,  \\nEternal beauty, where earth and sky convene.  \", 'thought': None, 'source': 'assistant_agent', 'type': 'AssistantMessage'}]}}, 'message_buffer': []}, 'RoundRobinGroupChatManager': {'type': 'RoundRobinManagerState', 'version': '1.0.0', 'message_thread': [{'source': 'user', 'models_usage': None, 'metadata': {}, 'content': 'Write a beautiful poem 3-line about lake tangayika', 'type': 'TextMessage'}, {'source': 'assistant_agent', 'models_usage': {'prompt_tokens': 29, 'completion_tokens': 37}, 'metadata': {}, 'content': \"Beneath the sun's embrace, Tanganyika gleams,  \\nWhispers of ancient waters weave through dreams,  \\nEternal beauty, where earth and sky convene.  \", 'type': 'TextMessage'}], 'current_turn': 0, 'next_speaker_index': 0}}}\n",
            "---------- user ----------\n",
            "What was the last line of the poem you wrote?\n",
            "---------- assistant_agent ----------\n",
            "The last line of the poem I wrote was:  \n",
            "\"Eternal beauty, where earth and sky convene.\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='What was the last line of the poem you wrote?', type='TextMessage'), TextMessage(source='assistant_agent', models_usage=RequestUsage(prompt_tokens=88, completion_tokens=23), metadata={}, content='The last line of the poem I wrote was:  \\n\"Eternal beauty, where earth and sky convene.\"', type='TextMessage')], stop_reason='Maximum number of messages 2 reached, current message count: 2')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persisting State (File or Database)\n",
        "In many cases, we may want to persist the state of the team to disk (or a database) and load it back later. State is a dictionary that can be serialized to a file or written to a database."
      ],
      "metadata": {
        "id": "1CuBLotZfFlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "## save state to disk\n",
        "# Create the 'content' directory if it doesn't exist\n",
        "os.makedirs(\"content\", exist_ok=True)\n",
        "\n",
        "with open(\"content/team_state.json\", \"w\") as f:\n",
        "    json.dump(team_state, f)\n",
        "\n",
        "## load state from disk\n",
        "with open(\"content/team_state.json\", \"r\") as f:\n",
        "    team_state = json.load(f)\n",
        "\n",
        "new_agent_team = RoundRobinGroupChat([assistant_agent], termination_condition=MaxMessageTermination(max_messages=2))\n",
        "await new_agent_team.load_state(team_state)\n",
        "stream = new_agent_team.run_stream(task=\"What was the last line of the poem you wrote?\")\n",
        "await Console(stream)\n",
        "await model_client.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IXWZxece8Ev",
        "outputId": "dcf474c2-55c9-4615-e6ab-db4d77d799c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "What was the last line of the poem you wrote?\n",
            "---------- assistant_agent ----------\n",
            "The last line of the poem I wrote for you was:  \n",
            "\"Eternal beauty, where earth and sky convene.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Agents\n",
        "You may have agents with behaviors that do not fall into a preset. In such cases, you can build custom agents.\n",
        "\n",
        "All agents in AgentChat inherit from BaseChatAgent class and implement the following abstract methods and attributes:\n",
        "\n",
        "on_messages(): The abstract method that defines the behavior of the agent in response to messages. This method is called when the agent is asked to provide a response in run(). It returns a Response object.\n",
        "\n",
        "on_reset(): The abstract method that resets the agent to its initial state. This method is called when the agent is asked to reset itself.\n",
        "\n",
        "produced_message_types: The list of possible BaseChatMessage message types the agent can produce in its response.\n",
        "\n",
        "Optionally, you can implement the the on_messages_stream() method to stream messages as they are generated by the agent. If this method is not implemented, the agent uses the default implementation of on_messages_stream() that calls the on_messages() method and yields all messages in the response."
      ],
      "metadata": {
        "id": "Wp7i86_jgQes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Sequence\n",
        "\n",
        "from autogen_agentchat.agents import BaseChatAgent\n",
        "from autogen_agentchat.base import Response\n",
        "from autogen_agentchat.conditions import MaxMessageTermination\n",
        "from autogen_agentchat.messages import BaseChatMessage, TextMessage\n",
        "from autogen_agentchat.teams import SelectorGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_core import CancellationToken\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "\n",
        "class ArithmeticAgent(BaseChatAgent):\n",
        "    def __init__(self, name: str, description: str, operator_func: Callable[[int], int]) -> None:\n",
        "        super().__init__(name, description=description)\n",
        "        self._operator_func = operator_func\n",
        "        self._message_history: List[BaseChatMessage] = []\n",
        "\n",
        "    @property\n",
        "    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:\n",
        "        return (TextMessage,)\n",
        "\n",
        "    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:\n",
        "        # Update the message history.\n",
        "        # NOTE: it is possible the messages is an empty list, which means the agent was selected previously.\n",
        "        self._message_history.extend(messages)\n",
        "        # Parse the number in the last message.\n",
        "        assert isinstance(self._message_history[-1], TextMessage)\n",
        "        number = int(self._message_history[-1].content)\n",
        "        # Apply the operator function to the number.\n",
        "        result = self._operator_func(number)\n",
        "        # Create a new message with the result.\n",
        "        response_message = TextMessage(content=str(result), source=self.name)\n",
        "        # Update the message history.\n",
        "        self._message_history.append(response_message)\n",
        "        # Return the response.\n",
        "        return Response(chat_message=response_message)\n",
        "\n",
        "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "ZGAdUmTtfKId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_number_agents() -> None:\n",
        "    # Create agents for number operations.\n",
        "    add_agent = ArithmeticAgent(\"add_agent\", \"Adds 1 to the number.\", lambda x: x + 1)\n",
        "    multiply_agent = ArithmeticAgent(\"multiply_agent\", \"Multiplies the number by 2.\", lambda x: x * 2)\n",
        "    subtract_agent = ArithmeticAgent(\"subtract_agent\", \"Subtracts 1 from the number.\", lambda x: x - 1)\n",
        "    divide_agent = ArithmeticAgent(\"divide_agent\", \"Divides the number by 2 and rounds down.\", lambda x: x // 2)\n",
        "    identity_agent = ArithmeticAgent(\"identity_agent\", \"Returns the number as is.\", lambda x: x)\n",
        "\n",
        "    # The termination condition is to stop after 10 messages.\n",
        "    termination_condition = MaxMessageTermination(10)\n",
        "\n",
        "    # Create a selector group chat.\n",
        "    selector_group_chat = SelectorGroupChat(\n",
        "        [add_agent, multiply_agent, subtract_agent, divide_agent, identity_agent],\n",
        "        model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"),\n",
        "        termination_condition=termination_condition,\n",
        "        allow_repeated_speaker=True,  # Allow the same agent to speak multiple times, necessary for this task.\n",
        "        selector_prompt=(\n",
        "            \"Available roles:\\n{roles}\\nTheir job descriptions:\\n{participants}\\n\"\n",
        "            \"Current conversation history:\\n{history}\\n\"\n",
        "            \"Please select the most appropriate role for the next message, and only return the role name.\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Run the selector group chat with a given task and stream the response.\n",
        "    task: List[BaseChatMessage] = [\n",
        "        TextMessage(content=\"Apply the operations to turn the given number into 25.\", source=\"user\"),\n",
        "        TextMessage(content=\"10\", source=\"user\"),\n",
        "    ]\n",
        "    stream = selector_group_chat.run_stream(task=task)\n",
        "    await Console(stream)\n",
        "\n",
        "\n",
        "# Use asyncio.run(run_number_agents()) when running in a script.\n",
        "await run_number_agents()\n"
      ],
      "metadata": {
        "id": "8oP6gJRWqlQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9e830a-4726-46d2-f54a-a9b27da8b363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "Apply the operations to turn the given number into 25.\n",
            "---------- user ----------\n",
            "10\n",
            "---------- add_agent ----------\n",
            "11\n",
            "---------- add_agent ----------\n",
            "12\n",
            "---------- add_agent ----------\n",
            "13\n",
            "---------- add_agent ----------\n",
            "14\n",
            "---------- add_agent ----------\n",
            "15\n",
            "---------- add_agent ----------\n",
            "16\n",
            "---------- add_agent ----------\n",
            "17\n",
            "---------- add_agent ----------\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selector Group Chat\n",
        "SelectorGroupChat implements a team where participants take turns broadcasting messages to all other members. A generative model (e.g., an LLM) selects the next speaker based on the shared context, enabling dynamic, context-aware collaboration.\n",
        "\n",
        "Key features include:\n",
        "\n",
        "Model-based speaker selection\n",
        "\n",
        "Configurable participant roles and descriptions\n",
        "\n",
        "Prevention of consecutive turns by the same speaker (optional)\n",
        "\n",
        "Customizable selection prompting\n",
        "\n",
        "Customizable selection function to override the default model-based selection\n",
        "\n",
        "Customizable candidate function to narrow-down the set of agents for selection using model"
      ],
      "metadata": {
        "id": "zeldLxStr5v5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How Does it Work?**\n",
        "SelectorGroupChat is a group chat similar to RoundRobinGroupChat, but with a model-based next speaker selection mechanism. When the team receives a task through run() or run_stream(), the following steps are executed:\n",
        "\n",
        "The team analyzes the current conversation context, including the conversation history and participants’ name and description attributes, to determine the next speaker using a model. By default, the team will not select the same speak consecutively unless it is the only agent available. This can be changed by setting allow_repeated_speaker=True. You can also override the model by providing a custom selection function.\n",
        "\n",
        "The team prompts the selected speaker agent to provide a response, which is then broadcasted to all other participants.\n",
        "\n",
        "The termination condition is checked to determine if the conversation should end, if not, the process repeats from step 1.\n",
        "\n",
        "When the conversation ends, the team returns the TaskResult containing the conversation history from this task.\n",
        "\n",
        "Once the team finishes the task, the conversation context is kept within the team and all participants, so the next task can continue from the previous conversation context. You can reset the conversation context by calling reset().\n",
        "\n",
        "In this section, we will demonstrate how to use SelectorGroupChat with a simple example for a web search and data analysis task."
      ],
      "metadata": {
        "id": "JLtOPFU9sQC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Sequence\n",
        "\n",
        "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
        "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
        "from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\n",
        "from autogen_agentchat.teams import SelectorGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient"
      ],
      "metadata": {
        "id": "CHzX5L8_rV0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: This example uses mock tools instead of real APIs for demonstration purposes\n",
        "def search_web_tool(query: str) -> str:\n",
        "    if \"2006-2007\" in query:\n",
        "        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
        "        Udonis Haslem: 844 points\n",
        "        Dwayne Wade: 1397 points\n",
        "        James Posey: 550 points\n",
        "        ...\n",
        "        \"\"\"\n",
        "    elif \"2007-2008\" in query:\n",
        "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n",
        "    elif \"2008-2009\" in query:\n",
        "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n",
        "    return \"No data found.\"\n",
        "\n",
        "\n",
        "def percentage_change_tool(start: float, end: float) -> float:\n",
        "    return ((end - start) / start) * 100\n"
      ],
      "metadata": {
        "id": "PZsiNMv5uxAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n",
        "\n",
        "planning_agent = AssistantAgent(\n",
        "    \"PlanningAgent\",\n",
        "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are a planning agent.\n",
        "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
        "    Your team members are:\n",
        "        WebSearchAgent: Searches for information\n",
        "        DataAnalystAgent: Performs calculations\n",
        "\n",
        "    You only plan and delegate tasks - you do not execute them yourself.\n",
        "\n",
        "    When assigning tasks, use this format:\n",
        "    1. <agent> : <task>\n",
        "\n",
        "    After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "web_search_agent = AssistantAgent(\n",
        "    \"WebSearchAgent\",\n",
        "    description=\"An agent for searching information on the web.\",\n",
        "    tools=[search_web_tool],\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "    You are a web search agent.\n",
        "    Your only tool is search_tool - use it to find information.\n",
        "    You make only one search call at a time.\n",
        "    Once you have the results, you never do calculations based on them.\n",
        "    \"\"\",\n",
        ")\n",
        "\n",
        "data_analyst_agent = AssistantAgent(\n",
        "    \"DataAnalystAgent\",\n",
        "    description=\"An agent for performing calculations.\",\n",
        "    model_client=model_client,\n",
        "    tools=[percentage_change_tool],\n",
        "    system_message=\"\"\"\n",
        "    You are a data analyst.\n",
        "    Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n",
        "    If you have not seen the data, ask for it.\n",
        "    \"\"\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "7RfdyhLnvYLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=25)\n",
        "termination = text_mention_termination | max_messages_termination\n"
      ],
      "metadata": {
        "id": "P5DzgKNzvb3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selector_prompt = \"\"\"Select an agent to perform task.\n",
        "\n",
        "{roles}\n",
        "\n",
        "Current conversation context:\n",
        "{history}\n",
        "\n",
        "Read the above conversation, then select an agent from {participants} to perform the next task.\n",
        "Make sure the planner agent has assigned tasks before other agents start working.\n",
        "Only select one agent.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "H9IoHnagvnIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "team = SelectorGroupChat(\n",
        "    [planning_agent, web_search_agent, data_analyst_agent],\n",
        "    model_client=model_client,\n",
        "    termination_condition=termination,\n",
        "    selector_prompt=selector_prompt,\n",
        "    allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n",
        ")"
      ],
      "metadata": {
        "id": "ofaA3JGNwrFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\""
      ],
      "metadata": {
        "id": "oVSnQRoNw3im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await Console(team.run_stream(task=task))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFOU95How7F_",
        "outputId": "00a51dcb-1c74-4d60-9b6b-a31c38203e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\n",
            "---------- PlanningAgent ----------\n",
            "To answer this complex task, we need to follow these steps:\n",
            "\n",
            "1. Identify the Miami Heat player with the highest points in the 2006-2007 season.\n",
            "2. Determine the total rebounds for the identified player in the 2007-2008 and 2008-2009 seasons.\n",
            "3. Calculate the percentage change in the total rebounds between these two seasons.\n",
            "\n",
            "Let's delegate these tasks accordingly:\n",
            "\n",
            "1. WebSearchAgent: Find out who was the Miami Heat player with the highest points in the 2006-2007 NBA season.\n",
            "2. WebSearchAgent: Once the player's name is identified, find out the total rebounds he secured during the 2007-2008 NBA season.\n",
            "3. WebSearchAgent: Find out the total rebounds the same player secured during the 2008-2009 NBA season.\n",
            "4. DataAnalystAgent: Calculate the percentage change in total rebounds for the player between the 2007-2008 and 2008-2009 seasons.\n",
            "---------- WebSearchAgent ----------\n",
            "[FunctionCall(id='call_SAO9BCn3K9jSbeSfKP0SjF9a', arguments='{\"query\":\"Miami Heat highest points player 2006-2007 season\"}', name='search_web_tool')]\n",
            "---------- WebSearchAgent ----------\n",
            "[FunctionExecutionResult(content='Here are the total points scored by Miami Heat players in the 2006-2007 season:\\n        Udonis Haslem: 844 points\\n        Dwayne Wade: 1397 points\\n        James Posey: 550 points\\n        ...\\n        ', name='search_web_tool', call_id='call_SAO9BCn3K9jSbeSfKP0SjF9a', is_error=False)]\n",
            "---------- WebSearchAgent ----------\n",
            "Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
            "        Udonis Haslem: 844 points\n",
            "        Dwayne Wade: 1397 points\n",
            "        James Posey: 550 points\n",
            "        ...\n",
            "        \n",
            "---------- WebSearchAgent ----------\n",
            "The player with the highest points for the Miami Heat in the 2006-2007 season was Dwyane Wade with 1,397 points.\n",
            "\n",
            "Next, we need to find Dwyane Wade's total rebounds in the 2007-2008 season.\n",
            "---------- WebSearchAgent ----------\n",
            "[FunctionCall(id='call_EHDQA1MkXwzPGbp3ikwZgBJ8', arguments='{\"query\":\"Dwyane Wade total rebounds 2007-2008 season\"}', name='search_web_tool')]\n",
            "---------- WebSearchAgent ----------\n",
            "[FunctionExecutionResult(content='The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.', name='search_web_tool', call_id='call_EHDQA1MkXwzPGbp3ikwZgBJ8', is_error=False)]\n",
            "---------- WebSearchAgent ----------\n",
            "The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\n",
            "---------- WebSearchAgent ----------\n",
            "Dwyane Wade had 214 total rebounds in the 2007-2008 season.\n",
            "\n",
            "Now, let's find out his total rebounds for the 2008-2009 season.\n",
            "---------- WebSearchAgent ----------\n",
            "[FunctionCall(id='call_KEVsRhIt2p44ltM47Oa4jXDx', arguments='{\"query\":\"Dwyane Wade total rebounds 2008-2009 season\"}', name='search_web_tool')]\n",
            "---------- WebSearchAgent ----------\n",
            "[FunctionExecutionResult(content='The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.', name='search_web_tool', call_id='call_KEVsRhIt2p44ltM47Oa4jXDx', is_error=False)]\n",
            "---------- WebSearchAgent ----------\n",
            "The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\n",
            "---------- DataAnalystAgent ----------\n",
            "[FunctionCall(id='call_Xu8PPUJmRvBuHpcIz7uoPJuX', arguments='{\"start\":214,\"end\":398}', name='percentage_change_tool')]\n",
            "---------- DataAnalystAgent ----------\n",
            "[FunctionExecutionResult(content='85.98130841121495', name='percentage_change_tool', call_id='call_Xu8PPUJmRvBuHpcIz7uoPJuX', is_error=False)]\n",
            "---------- DataAnalystAgent ----------\n",
            "85.98130841121495\n",
            "---------- PlanningAgent ----------\n",
            "The Miami Heat player with the highest points in the 2006-2007 season was Dwayne Wade, with 1,397 points. The percentage change in his total rebounds between the 2007-2008 season (214 rebounds) and 2008-2009 season (398 rebounds) is approximately an 86% increase. \n",
            "\n",
            "TERMINATE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?', type='TextMessage'), TextMessage(source='PlanningAgent', models_usage=RequestUsage(prompt_tokens=161, completion_tokens=203), metadata={}, content=\"To answer this complex task, we need to follow these steps:\\n\\n1. Identify the Miami Heat player with the highest points in the 2006-2007 season.\\n2. Determine the total rebounds for the identified player in the 2007-2008 and 2008-2009 seasons.\\n3. Calculate the percentage change in the total rebounds between these two seasons.\\n\\nLet's delegate these tasks accordingly:\\n\\n1. WebSearchAgent: Find out who was the Miami Heat player with the highest points in the 2006-2007 NBA season.\\n2. WebSearchAgent: Once the player's name is identified, find out the total rebounds he secured during the 2007-2008 NBA season.\\n3. WebSearchAgent: Find out the total rebounds the same player secured during the 2008-2009 NBA season.\\n4. DataAnalystAgent: Calculate the percentage change in total rebounds for the player between the 2007-2008 and 2008-2009 seasons.\", type='TextMessage'), ToolCallRequestEvent(source='WebSearchAgent', models_usage=RequestUsage(prompt_tokens=351, completion_tokens=27), metadata={}, content=[FunctionCall(id='call_SAO9BCn3K9jSbeSfKP0SjF9a', arguments='{\"query\":\"Miami Heat highest points player 2006-2007 season\"}', name='search_web_tool')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='WebSearchAgent', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='Here are the total points scored by Miami Heat players in the 2006-2007 season:\\n        Udonis Haslem: 844 points\\n        Dwayne Wade: 1397 points\\n        James Posey: 550 points\\n        ...\\n        ', name='search_web_tool', call_id='call_SAO9BCn3K9jSbeSfKP0SjF9a', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='WebSearchAgent', models_usage=None, metadata={}, content='Here are the total points scored by Miami Heat players in the 2006-2007 season:\\n        Udonis Haslem: 844 points\\n        Dwayne Wade: 1397 points\\n        James Posey: 550 points\\n        ...\\n        ', type='ToolCallSummaryMessage'), ThoughtEvent(source='WebSearchAgent', models_usage=None, metadata={}, content=\"The player with the highest points for the Miami Heat in the 2006-2007 season was Dwyane Wade with 1,397 points.\\n\\nNext, we need to find Dwyane Wade's total rebounds in the 2007-2008 season.\", type='ThoughtEvent'), ToolCallRequestEvent(source='WebSearchAgent', models_usage=RequestUsage(prompt_tokens=443, completion_tokens=84), metadata={}, content=[FunctionCall(id='call_EHDQA1MkXwzPGbp3ikwZgBJ8', arguments='{\"query\":\"Dwyane Wade total rebounds 2007-2008 season\"}', name='search_web_tool')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='WebSearchAgent', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.', name='search_web_tool', call_id='call_EHDQA1MkXwzPGbp3ikwZgBJ8', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='WebSearchAgent', models_usage=None, metadata={}, content='The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.', type='ToolCallSummaryMessage'), ThoughtEvent(source='WebSearchAgent', models_usage=None, metadata={}, content=\"Dwyane Wade had 214 total rebounds in the 2007-2008 season.\\n\\nNow, let's find out his total rebounds for the 2008-2009 season.\", type='ThoughtEvent'), ToolCallRequestEvent(source='WebSearchAgent', models_usage=RequestUsage(prompt_tokens=569, completion_tokens=67), metadata={}, content=[FunctionCall(id='call_KEVsRhIt2p44ltM47Oa4jXDx', arguments='{\"query\":\"Dwyane Wade total rebounds 2008-2009 season\"}', name='search_web_tool')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='WebSearchAgent', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.', name='search_web_tool', call_id='call_KEVsRhIt2p44ltM47Oa4jXDx', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='WebSearchAgent', models_usage=None, metadata={}, content='The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.', type='ToolCallSummaryMessage'), ToolCallRequestEvent(source='DataAnalystAgent', models_usage=RequestUsage(prompt_tokens=479, completion_tokens=21), metadata={}, content=[FunctionCall(id='call_Xu8PPUJmRvBuHpcIz7uoPJuX', arguments='{\"start\":214,\"end\":398}', name='percentage_change_tool')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='DataAnalystAgent', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='85.98130841121495', name='percentage_change_tool', call_id='call_Xu8PPUJmRvBuHpcIz7uoPJuX', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='DataAnalystAgent', models_usage=None, metadata={}, content='85.98130841121495', type='ToolCallSummaryMessage'), TextMessage(source='PlanningAgent', models_usage=RequestUsage(prompt_tokens=511, completion_tokens=74), metadata={}, content='The Miami Heat player with the highest points in the 2006-2007 season was Dwayne Wade, with 1,397 points. The percentage change in his total rebounds between the 2007-2008 season (214 rebounds) and 2008-2009 season (398 rebounds) is approximately an 86% increase. \\n\\nTERMINATE', type='TextMessage')], stop_reason=\"Text 'TERMINATE' mentioned\")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiofiles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qbh64sa_EJGH",
        "outputId": "553145d9-20d3-4c9a-98f3-3785ab5b3c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiofiles\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: aiofiles\n",
            "Successfully installed aiofiles-24.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb==0.3.21"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cLCCzjEgEW1X",
        "outputId": "7d104466-0eb2-4086-b861-781dc1dcfac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chromadb==0.3.21\n",
            "  Downloading chromadb-0.3.21-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.3.21) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.3.21) (2.32.3)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.3.21) (2.11.2)\n",
            "Collecting hnswlib>=0.7 (from chromadb==0.3.21)\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb==0.3.21)\n",
            "  Downloading clickhouse_connect-0.8.17-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.3.21) (3.4.1)\n",
            "Requirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.3.21) (1.2.1)\n",
            "Requirement already satisfied: fastapi>=0.85.1 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.3.21) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.3.21) (2.0.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.3.21) (3.24.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2025.1.31)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.11/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2025.2)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (0.23.0)\n",
            "Collecting lz4 (from clickhouse-connect>=0.5.7->chromadb==0.3.21)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.85.1->chromadb==0.3.21) (0.45.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.85.1->chromadb==0.3.21) (4.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->chromadb==0.3.21) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3->chromadb==0.3.21) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.17.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.3.21) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.3.21) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.3.21) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.3.21) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.3.21) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.3.21) (3.10)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.30.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (11.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.21) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (1.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (6.0.2)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (4.9.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi>=0.85.1->chromadb==0.3.21) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.0.2)\n",
            "Downloading chromadb-0.3.21-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading clickhouse_connect-0.8.17-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp311-cp311-linux_x86_64.whl size=2383983 sha256=069bc915ffe159f54404eed8eb18606847f2b29296c80dd35d283f21e6f59d0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/4e/27/39aebca9958719776e36fada290845a7ef10f053ad70e22ceb\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lz4, hnswlib, nvidia-cusparse-cu12, nvidia-cudnn-cu12, clickhouse-connect, nvidia-cusolver-cu12, chromadb\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: chromadb\n",
            "    Found existing installation: chromadb 1.0.4\n",
            "    Uninstalling chromadb-1.0.4:\n",
            "      Successfully uninstalled chromadb-1.0.4\n",
            "Successfully installed chromadb-0.3.21 clickhouse-connect-0.8.17 hnswlib-0.8.0 lz4-4.4.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chromadb"
                ]
              },
              "id": "2e8d9b8a1cb041c7a16cf0d6ce860cd1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List\n",
        "\n",
        "import aiofiles\n",
        "import aiohttp\n",
        "from autogen_core.memory import Memory, MemoryContent, MemoryMimeType\n",
        "\n",
        "\n",
        "class SimpleDocumentIndexer:\n",
        "    \"\"\"Basic document indexer for AutoGen Memory.\"\"\"\n",
        "\n",
        "    def __init__(self, memory: Memory, chunk_size: int = 1500) -> None:\n",
        "        self.memory = memory\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    async def _fetch_content(self, source: str) -> str:\n",
        "        \"\"\"Fetch content from URL or file.\"\"\"\n",
        "        if source.startswith((\"http://\", \"https://\")):\n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                async with session.get(source) as response:\n",
        "                    return await response.text()\n",
        "        else:\n",
        "            async with aiofiles.open(source, \"r\", encoding=\"utf-8\") as f:\n",
        "                return await f.read()\n",
        "\n",
        "    def _strip_html(self, text: str) -> str:\n",
        "        \"\"\"Remove HTML tags and normalize whitespace.\"\"\"\n",
        "        text = re.sub(r\"<[^>]*>\", \" \", text)\n",
        "        text = re.sub(r\"\\s+\", \" \", text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _split_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into fixed-size chunks.\"\"\"\n",
        "        chunks: list[str] = []\n",
        "        # Just split text into fixed-size chunks\n",
        "        for i in range(0, len(text), self.chunk_size):\n",
        "            chunk = text[i : i + self.chunk_size]\n",
        "            chunks.append(chunk.strip())\n",
        "        return chunks\n",
        "\n",
        "    async def index_documents(self, sources: List[str]) -> int:\n",
        "        \"\"\"Index documents into memory.\"\"\"\n",
        "        total_chunks = 0\n",
        "\n",
        "        for source in sources:\n",
        "            try:\n",
        "                content = await self._fetch_content(source)\n",
        "\n",
        "                # Strip HTML if content appears to be HTML\n",
        "                if \"<\" in content and \">\" in content:\n",
        "                    content = self._strip_html(content)\n",
        "\n",
        "                chunks = self._split_text(content)\n",
        "\n",
        "                for i, chunk in enumerate(chunks):\n",
        "                    await self.memory.add(\n",
        "                        MemoryContent(\n",
        "                            content=chunk, mime_type=MemoryMimeType.TEXT, metadata={\"source\": source, \"chunk_index\": i}\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                total_chunks += len(chunks)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error indexing {source}: {str(e)}\")\n",
        "\n",
        "        return total_chunks\n"
      ],
      "metadata": {
        "id": "LQTLn8EPEQ7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.ui import Console\n",
        "from autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "# Initialize vector memory\n",
        "\n",
        "rag_memory = ChromaDBVectorMemory(\n",
        "    config=PersistentChromaDBVectorMemoryConfig(\n",
        "        collection_name=\"autogen_docs\",\n",
        "        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n",
        "        k=3,  # Return top 3 results\n",
        "        score_threshold=0.4,  # Minimum similarity score\n",
        "    )\n",
        ")\n",
        "\n",
        "await rag_memory.clear()  # Clear existing memory\n",
        "\n",
        "\n",
        "# Index AutoGen documentation\n",
        "async def index_autogen_docs() -> None:\n",
        "    indexer = SimpleDocumentIndexer(memory=rag_memory)\n",
        "    sources = [\n",
        "        \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
        "        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/agents.html\",\n",
        "        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/teams.html\",\n",
        "        \"https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/termination.html\",\n",
        "    ]\n",
        "    chunks: int = await indexer.index_documents(sources)\n",
        "    print(f\"Indexed {chunks} chunks from {len(sources)} AutoGen documents\")\n",
        "\n",
        "\n",
        "await index_autogen_docs()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "7XiNb0XUFcT9",
        "outputId": "32abe75f-a4ac-432a-948d-7630ec97f1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'IncludeEnum' from 'chromadb.api.types' (/usr/local/lib/python3.11/dist-packages/chromadb/api/types.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2a03423f8bfe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautogen_agentchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAssistantAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautogen_agentchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConsole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautogen_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchromadb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChromaDBVectorMemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPersistentChromaDBVectorMemoryConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautogen_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIChatCompletionClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen_ext/memory/chromadb.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCollection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIncludeEnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'IncludeEnum' from 'chromadb.api.types' (/usr/local/lib/python3.11/dist-packages/chromadb/api/types.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "trR8yiT8VOSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
        "from chromadb.utils import embedding_functions\n",
        "import chromadb\n",
        "\n",
        "\n",
        "model_client = OpenAIChatCompletionClient(\n",
        "    model=\"gpt-4o\",\n",
        "    api_key=os.environ['OPENAI_API_KEY']\n",
        ")\n",
        "\n",
        "# Create ChromaDB collection\n",
        "client = chromadb.Client()\n",
        "embedding_func = embedding_functions.DefaultEmbeddingFunction()\n",
        "collection = client.create_collection(name=\"knowledge_base\", embedding_function=embedding_func)\n",
        "\n",
        "# Add some documents to the knowledge base\n",
        "documents = [\n",
        "    \"AutoGen is a framework for developing LLM applications.\",\n",
        "    \"RAG stands for Retrieval-Augmented Generation.\",\n",
        "    \"ChromaDB is a vector database for embeddings.\",\n",
        "    \"You can use AutoGen to build multi-agent systems.\"\n",
        "]\n",
        "collection.add(\n",
        "    documents=documents,\n",
        "    ids=[f\"doc_{i}\" for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "# Create agents\n",
        "assistant = AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"You are a helpful assistant. Use the provided functions to retrieve information when needed.\"\n",
        ")\n",
        "\n",
        "user_proxy = UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    human_input_mode=\"ALWAYS\",\n",
        "    code_execution_config=False,\n",
        "    default_auto_reply=\"Please continue if you have more questions.\"\n",
        ")\n",
        "\n",
        "# Register retrieval function\n",
        "def retrieve_information(query):\n",
        "    results = collection.query(query_texts=[query], n_results=2)\n",
        "    return \"\\n\".join(results[\"documents\"][0])\n",
        "\n",
        "# Register the function with both agents\n",
        "user_proxy.register_function(\n",
        "    function_map={\n",
        "        \"retrieve_information\": retrieve_information\n",
        "    }\n",
        ")\n",
        "\n",
        "assistant.register_function(\n",
        "    function_map={\n",
        "        \"retrieve_information\": retrieve_information\n",
        "    }\n",
        ")\n",
        "\n",
        "# Start conversation\n",
        "def start_chat():\n",
        "    user_proxy.initiate_chat(\n",
        "        assistant,\n",
        "        message=\"What is AutoGen?\"\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "0JXZHIY4FiL_",
        "outputId": "c121cd54-1dda-4780-e2ef-9f037f1c3950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "PydanticImportError",
          "evalue": "`BaseSettings` has been moved to the `pydantic-settings` package. See https://docs.pydantic.dev/2.11/migration/#basesettings-has-moved-to-pydantic-settings for more details.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/import-error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPydanticImportError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f2f692ff0aa0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautogen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAssistantAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserProxyAgent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_list_from_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtelemetry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClientStartEvent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtelemetry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposthog\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPosthog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/chromadb/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mTELEMETRY_WHITELISTED_SETTINGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"chroma_db_impl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chroma_api_impl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"chroma_server_ssl_enabled\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mdynamic_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dynamic_imports\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdynamic_attr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_getattr_migration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamic_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/_migration.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimport_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREDIRECT_TO_V1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimport_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimport_path\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pydantic:BaseSettings'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             raise PydanticImportError(\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0;34m'`BaseSettings` has been moved to the `pydantic-settings` package. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;34mf'See https://docs.pydantic.dev/{version_short()}/migration/#basesettings-has-moved-to-pydantic-settings '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPydanticImportError\u001b[0m: `BaseSettings` has been moved to the `pydantic-settings` package. See https://docs.pydantic.dev/2.11/migration/#basesettings-has-moved-to-pydantic-settings for more details.\n\nFor further information visit https://errors.pydantic.dev/2.11/u/import-error",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ojy4utlAWbHS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}