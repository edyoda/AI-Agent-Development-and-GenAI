{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edyoda/AI-Agent-Development-and-GenAI/blob/main/Day2_GenAI_Foundations_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBm5V3Laxcjz"
      },
      "source": [
        "To understand the foundations of Generative AI (GenAI), it's important to start with the basics of machine learning, deep learning, and neural networks. Below is a Python code example that demonstrates the foundational concepts of Generative AI using a simple Generative Adversarial Network (GAN). This example will help you understand how generative models work.\n",
        "\n",
        "**Step 1: Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY9S4OTAFUAK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoT3FLL7xVHn"
      },
      "source": [
        "**Step 2: Define the Generator and Discriminator**\n",
        "\n",
        "The generator creates fake data, and the discriminator tries to distinguish between real and fake data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rmLtMpIHeUw"
      },
      "outputs": [],
      "source": [
        "# Generator model\n",
        "def build_generator(latent_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(128, input_dim=latent_dim, activation='relu'),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(784, activation='sigmoid'),  # 28x28 for MNIST images\n",
        "        layers.Reshape((28, 28, 1))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Discriminator model\n",
        "def build_discriminator(img_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Flatten(input_shape=img_shape),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')  # Binary classification (real/fake)\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_BenqEIx8_e"
      },
      "source": [
        "**Step 3: Define the GAN**\n",
        "\n",
        "Combine the generator and discriminator into a GAN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrWVjsETx4bL"
      },
      "outputs": [],
      "source": [
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False  # Freeze discriminator during generator training\n",
        "    model = tf.keras.Sequential([\n",
        "        generator,\n",
        "        discriminator\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn1wAKU2yLMC"
      },
      "source": [
        "**Step 4: Load and Preprocess Data**\n",
        "\n",
        "For this example, we'll use the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfHktI76yGup"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "(X_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train / 255.0  # Normalize to [0, 1]\n",
        "X_train = np.expand_dims(X_train, axis=-1)  # Add channel dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owFwbBPWyWb2"
      },
      "source": [
        "**Step 5: Compile Models**\n",
        "\n",
        "Compile the discriminator and GAN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSRpjbGlyQNM"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "img_shape = (28, 28, 1)\n",
        "\n",
        "# Build and compile discriminator\n",
        "discriminator = build_discriminator(img_shape)\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build generator\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "# Build and compile GAN\n",
        "gan = build_gan(generator, discriminator)\n",
        "gan.compile(optimizer='adam', loss='binary_crossentropy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccD1LwQazG0Z"
      },
      "source": [
        "**Step 6: Train the GAN**\n",
        "\n",
        "Train the GAN in alternating steps: train the discriminator on real and fake data, then train the generator to fool the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5grk9yyybLD",
        "outputId": "1f83696b-58d3-471c-d81d-f0559b15af78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Epoch: 0, D Loss: 1.6128430366516113, G Loss: 0.20828667283058167\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Epoch: 1, D Loss: 1.6502070426940918, G Loss: 0.19951413571834564\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Epoch: 2, D Loss: 1.686079978942871, G Loss: 0.19146983325481415\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Epoch: 3, D Loss: 1.7197588682174683, G Loss: 0.1840730458498001\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Epoch: 4, D Loss: 1.7511080503463745, G Loss: 0.17724888026714325\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Epoch: 5, D Loss: 1.7808496952056885, G Loss: 0.17093497514724731\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Epoch: 6, D Loss: 1.8089277744293213, G Loss: 0.16507670283317566\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Epoch: 7, D Loss: 1.8354817628860474, G Loss: 0.15962518751621246\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Epoch: 8, D Loss: 1.860532522201538, G Loss: 0.15453845262527466\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Epoch: 9, D Loss: 1.8843941688537598, G Loss: 0.14978012442588806\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Train discriminator\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    real_imgs = X_train[idx]\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    fake_imgs = generator.predict(noise)\n",
        "\n",
        "    # Labels for real and fake images\n",
        "    real_labels = np.ones((batch_size, 1))\n",
        "    fake_labels = np.zeros((batch_size, 1))\n",
        "\n",
        "    # Train discriminator on real and fake images\n",
        "    d_loss_real = discriminator.train_on_batch(real_imgs, real_labels)\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_labels)\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    g_loss = gan.train_on_batch(noise, real_labels)  # Generator tries to fool discriminator\n",
        "\n",
        "    # Print progress\n",
        "\n",
        "    print(f\"Epoch: {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
        "    # Generate and save sample images\n",
        "    sample_images(generator, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMXUpsN8zamU"
      },
      "source": [
        "**Step 7: Generate Sample Images**\n",
        "\n",
        "Define a function to generate and save sample images during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMp8O5hXzNH4"
      },
      "outputs": [],
      "source": [
        "def sample_images(generator, epoch, n=5):\n",
        "    noise = np.random.normal(0, 1, (n * n, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images to [0, 1]\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(n, n)\n",
        "    cnt = 0\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(f\"gan_images/epoch_{epoch}.png\")\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxo6Um810X0Z"
      },
      "source": [
        "**Key Concepts in This Code:**\n",
        "\n",
        "Generator: Creates fake data (e.g., images).\n",
        "\n",
        "Discriminator: Distinguishes between real and fake data.\n",
        "\n",
        "Adversarial Training: The generator and discriminator are trained simultaneously in a competitive manner.\n",
        "\n",
        "Latent Space: The generator uses random noise (latent vectors) to create data.\n",
        "\n",
        "Loss Functions: Binary cross-entropy is used to measure how well the discriminator and generator perform.\n",
        "\n",
        "This is a basic implementation of a GAN. Modern Generative AI models like GPT, DALL·E, and Stable Diffusion build on these foundational concepts but use more advanced architectures and techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVqCyDBfzg9v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzd0nttV/7g7XV2qGhdtEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}